{
    "Books": [
        {
            "title": "Deep learning techniques for music generation--a survey",
            "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Deep+Learning+Techniques+for+Music+Generation+%E2%80%93+A+Survey&btnG=", 
            "content_link": "https://arxiv.org/pdf/1709.01620.pdf",
            "quick_notes": "This is an entire book about deep learning for music generation. I've read everything up to the network architectures section. This has been the most informative resource that I have used so far.", 
            "citation": "Briot, J. P., Hadjeres, G., & Pachet, F. D. (2017). Deep learning techniques for music generation--a survey. arXiv preprint arXiv:1709.01620."
        },
        {
            "title": "Deep Learning Book",
            "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=deep+learning+goodfellow&btnG=", 
            "content_link": "https://github.com/janishar/mit-deep-learning-book-pdf",
            "quick_notes": "The famous book by goodfellow. It is dense enough that I haven't read that much of it, but think that it would be useful to look at the section on RNNs and LSTMs", 
            "citation": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
        }
    ], 
    "Papers": {
        "AI Tutor": [
            {
                "title": "Toward a High Performance Piano Practice Support System for Beginners",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Toward+a+High+Performance+Piano+Practice+Support+System+for+Beginners&btnG=",
                "content_link": "http://www.apsipa.org/proceedings/2018/pdfs/0000073.pdf",
                "quick_notes": "This appears to be the closest application to what I want to do. The paper is difficult to read as the english doesn't make much sense in many places (due to the native language of the authors), but the overall idea is as close to what I want to do as anything else I could find.",
                "citation": "Asahi, Shota & Tamura, Satoshi & Sugiyama, Yuko & Hayamizu, Satoru. (2018). Toward a High Performance Piano Practice Support System for Beginners. 73-79. 10.23919/APSIPA.2018.8659463.",
                "review_link": "",
                "website_link": ""
            }, 
            {
                "title": "Detecting Pianist Hand Posture Mistakes for Virtual Piano Tutoring",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Detecting+Pianist+Hand+Posture+Mistakes+for+Virtual+Piano+Tutoring&btnG=",
                "content_link": "https://www.researchgate.net/profile/David_Johnson143/publication/318028952_Detecting_Pianist_Hand_Posture_Mistakes_for_Virtual_Piano_Tutoring/links/596042100f7e9b8194fc1119/Detecting-Pianist-Hand-Posture-Mistakes-for-Virtual-Piano-Tutoring.pdf",
                "quick_notes": "This is not 100% relevant to the research that I am doing but is a fascinating idea. It takes a computer vision based approach to tutoring by looking at hand position while playing. This isn't relevant to the research that I am doing but may be in the future.",
                "citation": "Johnson, David & Dufour, Isabelle & Damian, Daniela & Tzanetakis, George. (2016). Detecting Pianist Hand Posture Mistakes for Virtual Piano Tutoring. ",
                "review_link": "",
                "website_link": ""
            },
            {
                "title": "Computer-Assisted Musical Instrument Tutoring with Targeted Exercises",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Computer-Assisted+Musical+Instrument+Tutoring+with+Targeted+Exercises&btnG=",
                "content_link": "http://dspace.library.uvic.ca/bitstream/handle/1828/1081/masters-percival.pdf?sequence=1&isAllowed=y",
                "quick_notes": "This is a masters thesis and talks about computer music approaches for tutoring. It doesn't use any sophisticated deep learning models if I remember but does present some of the problems that are associated with computer based tutoring applications. I haven't read the whole paper in detail, but would like to better understand my own problem that I am trying to solve. ",
                "citation": "Percival, G. K. (2008). Computer-assisted musical instrument tutoring with targeted exercises (Doctoral dissertation).",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Score-Following-in-Practice",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Score-Following-in-Practice&btnG=",
                "content_link": "https://www.researchgate.net/profile/Cort_Lippe/publication/301649731_Score-Following-in-Practice/links/571fadd508aefa64889a811e.pdf",
                "quick_notes": "An older paper (1992) about score following. Not sure what the computational techniques that they use are",
                "citation": "Puckette, M., & Lippe, C. (1992). Score following in practice. In Proceedings of the International Computer Music Conference (pp. 182-182). INTERNATIONAL COMPUTER MUSIC ACCOCIATION.",
                "review_link": "",
                "website_link": ""
            
            }
        ], 
        "Music Analysis and Expressivness": {
            "General": [
                {
                    "title": "Computational Models of Expressive Music Performance: A Comprehensive and Critical Review",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Computational+Models+of+Expressive+Music+Performance%3A+A+Comprehensive+and+Critical+Review&btnG=",
                    "content_link": "https://www.frontiersin.org/articles/10.3389/fdigh.2018.00025/full",
                    "quick_notes": "A comprehensive review of music analysis and expressive performance. ",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                }, 
                {
                    "title": "A Hierarchical RNN-based System for Modeling Expressive Piano Performances",
                    "google_scholar_link": "https://scholar.google.com/scholar?q=A+Hierarchical+RNN-based+System+for+Modeling+Expressive+Piano+Performances&hl=en&as_sdt=0&as_vis=1&oi=scholart",
                    "content_link": "http://archives.ismir.net/ismir2019/paper/000112.pdf",
                    "quick_notes": "Uses a hierarchical RNN to build an expressive piano player. Will be the basis for most of the thesis. This is a very similar paper to the one below, but gives more of a broad overview of the entire system as opposed to specific details about the network architecture and feature represetation",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "VirtuosoNet: A Hierarchical Attention RNN for Generating Expressive Piano Performance from Music Score",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=VirtuosoNet%3A+A+Hierarchical+Attention+RNN+for+Generating+Expressive+Piano+Performance+from+Music+Score&btnG=",
                    "content_link": "https://nips2018creativity.github.io/doc/virtuosonet.pdf",
                    "quick_notes": "This is a related paper to the one above, but provides more specific details about the HAN and the feature representation",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "An Assessment of Learned Score Features for Modeling Expressive Dynamics in Music",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=An+Assessment+of+Learned+Score+Features+for+Modeling+Expressive+Dynamics+in+Music&btnG=",
                    "content_link": "https://ieeexplore-ieee-org.ezproxy.lib.utah.edu/stamp/stamp.jsp?tp=&arnumber=6762927",
                    "quick_notes": "Paper about using unsupervised learning to learn score features. ",
                    "citation": "",
                    "review_link": "{{<ref \"research/reviews/an-assessment-of-learned-score-features-for-modeling-expressive-dynamics-in-music.md\">}}",
                    "website_link": ""
                
                }, 
                {
                    "title": "Linear basis models for prediction and analysis of musical expression",
                    "google_scholar_link": "https://scholar.google.com/scholar_lookup?author=M.+Grachten&author=G.+Widmer+&publication_year=2012&title=Linear+basis+models+for+prediction+and+analysis+of+musical+expression&journal=J.+New+Music+Res.&volume=41&pages=311-322",
                    "content_link": "http://www.cp.jku.at/research/papers/Grachten_Widmer-JNMR-LBM-preprint.pdf",
                    "quick_notes": "Introduction to the linear basis model using simple linear regression. ",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "The Basis Mixer: A Computation Romantic Pianist",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=The+basis+mixer%3A+a+computational+romantic+pianist&btnG=",
                    "content_link": "https://s18798.pcdn.co/ismir2016/wp-content/uploads/sites/2294/2016/08/cancino-basis.pdf",
                    "quick_notes": "An extension of the basis model using an RNN. ",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "YQX Plays Chopin",
                    "google_scholar_link": "https://scholar.google.com/scholar_lookup?author=G.+Widmer&author=S.+Flossmann&author=M.+Grachten+&publication_year=2009&title=YQX+plays+chopin&journal=AI+Mag.&volume=30&pages=35-48",
                    "content_link": "https://www.aaai.org/ojs/index.php/aimagazine/article/view/2249/2099",
                    "quick_notes": "The YQX expressive performance rendering system, which was one piano competitions for expressive performance in 2008. The system uses a Bayesian probability network.",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "E-LEARNING SOFTWARE FOR IMPROVING STUDENT'S MUSIC PERFORMANCE USING COMPARISONS",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=E-LEARNING+SOFTWARE+FOR+IMPROVING+STUDENT%27S+MUSIC+PERFORMANCE+USING+COMPARISONS&btnG=",
                    "content_link": "https://files.eric.ed.gov/fulltext/ED562302.pdf",
                    "quick_notes": "This approach analyzes expresiveness by comparing a student piece to a professional. I have read into the paper too much but it is definitely worth looking into. I would probably try to do something similar for my application",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                }, 
                {
                    "title": "Expressive Collaborative Music Performance via Machine Learning",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Expressive+Collaborative+Music+Performance+via+Machine+Learning&btnG=",
                    "content_link": "http://reports-archive.adm.cs.cmu.edu/anon/anon/ml2016/CMU-ML-16-103.pdf",
                    "quick_notes": "This is a PhD Thesis which explores music performance from a computation perspective. There is quite a bit about musical representation, and he even touches on using computer vision to assist in HCI tools for music performance. The thesis is long, but has a substantial amount of good content",
                    "citation": "Xia, G. (2016). Expressive Collaborative Music Performance via Machine Learning.",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "A Phylogenetic Approach to Music Performance Analysis",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+Phylogenetic+Approach+to+Music+Performance+Analysis&btnG=",
                    "content_link": "http://www.cs.utexas.edu/users/eladlieb/paper.pdf",
                    "quick_notes": "Provides an alternative approach to music expressiveness. Focuses on violin as opposed to piano",
                    "citation": "Liebman, Elad & Ornoy, Eitan & Chor, Benny. (2012). A Phylogenetic Approach to Music Performance Analysis. Journal of New Music Research. 41. 195-222. 10.1080/09298215.2012.668194. ",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "A survey of computer systems for expressive music performance",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+survey+of+computer+systems+for+expressive+music+performance&btnG=",
                    "content_link": "https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/ComputerSystemsForMusic.pdf",
                    "quick_notes": "This is an older paper and forms much of the basis for expressive music performance analysis from a computational perspective. ",
                    "citation": "Alexis Kirke and Eduardo Reck Miranda. 2009. A survey of computer systems for expressive music performance. ACM Comput. Surv. 42, 1, Article 3 (December 2009), 41 pages. DOI:https://doi.org/10.1145/1592451.1592454",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "Modeling the Rational Basis of Musical Expression",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Modeling+the+Rational+Basis+of+Musical+Expression&btnG=",
                    "content_link": "https://www-jstor-org.ezproxy.lib.utah.edu/stable/pdf/3680601.pdf?ab_segments=0%2Fbasic_SYC-5187%2Fcontrol&refreqid=search%3A6b2d90946ee5b224cb0eb07d430fe170",
                    "quick_notes": "An early paper by widmer that describes a detailed model for his individual note level expressiveness model",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "On the Potential of Machine Learning for Music Research",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=On+the+Potential+of+Machine+Learning+for+Music+Research&btnG=",
                    "content_link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.874&rep=rep1&type=pdf",
                    "quick_notes": "This is an older paper by widmer where he introduces the idea of using machine learning for music research. I have read a good amount of the paper but need to go back through it to see what the exact algorithms and representation he is using. ",
                    "citation": "Widmer, G. (2013). On the potential of machine learning for music research. In Readings in Music and Artificial Intelligence (pp. 79-94). Routledge.",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "Overview of the KTH rule system for musical performance",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Overview+of+the+KTH+rule+system+for+musical+performance&btnG=",
                    "content_link": "http://ac-psych.org/en/download-pdf/page/20/volume/2/issue/2/id/15",
                    "quick_notes": "This is a popular paper by Friberg that discusses his KTH system for analyzing music expresiveness that he has been developing since the 80's. This doesn't use any sort of data driven approach but uses a set of hardcoded rules that are fine tuned by human feedback to define what expresiveness is. ",
                    "citation": "Friberg, A., Bresin, R., & Sundberg, J. (2006). Overview of the KTH rule system for musical performance. Advances in cognitive psychology, 2(2-3), 145-161.",
                    "review_link": "",
                    "website_link": ""
                
                }
            ], 
            "Transformer": [
                {
                    "title": "A BI-DIRECTIONAL TRANSFORMER FOR MUSICAL CHORD RECOGNITION",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+BI-DIRECTIONAL+TRANSFORMER+FOR+MUSICAL+CHORD+RECOGNITION&btnG=",
                    "content_link": "https://arxiv.org/pdf/1907.02698.pdf",
                    "quick_notes": "Uses the self-attention technique for chord recognition and provides an accompanying visualization",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }
            ]
        }, 
        "Music Generation": {
            "General": [
                {
                    "title": "Deep Learning for Music Generation - Challenges and Directions",
                    "google_scholar_link": "https://scholar.google.com/scholar?q=Deep+learning+for+music+generation:+challenges+and+directions&hl=en&as_sdt=0&as_vis=1&oi=scholart",
                    "content_link": "http://www-desir.lip6.fr/~briot/cv/mg-dl-cd-final.pdf",
                    "quick_notes": "A survey of the current trends in music generation and suggestions of future work",
                    "citation": "",
                    "review_link": "/research/reviews/onsets-and-frames-dual-objective-piano-transcription/",
                    "website_link": ""
                }, 
                {
                    "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Enabling+Factorized+Piano+Music+Modeling+and+Generation+with+the+MAESTRO+Dataset&btnG=",
                    "content_link": "https://arxiv.org/pdf/1810.12247.pdf",
                    "quick_notes": "A full audio transcription and generation process proposed by Magenta. They break the problem of generating music audio into 3 parts.",
                    "citation": "",
                    "review_link": "/research/reviews/enabling-factorized-piano-music-modeling-and-generation-with-the-maestro-dataset.md/",
                    "website_link": "https://magenta.tensorflow.org/maestro-wave2midi2wave"
                
                }
            ],
            "Transformer": [
                {
                    "title": "Music Transformer: Generating music with long-term structure",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Music+transformer%3A+Generating+music+with+long-term+structure&btnG=",
                    "content_link": "https://openreview.net/pdf?id=rJe4ShAcF7",
                    "quick_notes": "Magenta's(Google) take on using transformers to model long term relationships in music",
                    "citation": "",
                    "review_link": "",
                    "website_link": "https://magenta.tensorflow.org/music-transformer"
                
                }, 
                {
                    "title": "Music Style Transformer",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Music+Style+Transformer&btnG=",
                    "content_link": "https://csce.ucmss.com/cr/books/2019/LFS/CSREA2019/ICA7029.pdf",
                    "quick_notes": "Applying Magenta's research with a Style Transfer approach",
                    "citation": "",
                    "review_link": "/research/reviews/music-style-transformer/",
                    "website_link": ""
                
                },
                {
                    "title": "Encoding Musical Style with Transformer Autoencoders",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Encoding+Musical+Style+with+Transformer+Autoencoders&btnG=",
                    "content_link": "https://arxiv.org/pdf/1912.05537.pdf",
                    "quick_notes": "Magenta research group using an autoencoder on top of the music transformer architecture",
                    "citation": "",
                    "review_link": "",
                    "website_link": "https://magenta.tensorflow.org/transformer-autoencoder"
                
                },
                {
                    "title": "Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Transformer+VAE%3A+A+Hierarchical+Model+for+Structure-Aware+and+Interpretable+Music+Representation+Learning&btnG=",
                    "content_link": "https://ieeexplore-ieee-org.ezproxy.lib.utah.edu/stamp/stamp.jsp?tp=&arnumber=9054554",
                    "quick_notes": "Another variational auto-encoder with the goal to make create more structure awareness and interpretability in the model.",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                }, 
                {
                    "title": "LakhNES: Improving multi-instrumental music generation with cross-domain pre-training",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=LakhNES%3A+Improving+multi-instrumental+music+generation+with+cross-domain+pre-training&btnG=",
                    "content_link": "https://arxiv.org/pdf/1907.04868.pdf",
                    "quick_notes": "Using the transformer architecture for polyphonic music generation using new data from NES video games system. ",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "Transformer Bard: Music and Poem Generation Using Transformer Models",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Transformer+Bard%3A+Music+and+Poem+Generation+Using+Transformer+Models&btnG=",
                    "content_link": "https://creativecoding.soe.ucsc.edu/courses/cmpm202_w20/_schedule/TransformerBard_202.pdf",
                    "quick_notes": "Uses the GPT-2 Model for text generation and the Self-Attention Transformer model for music generation",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "Vector Quantized Contrastive Predictive Coding for Template-based Music Generation",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Vector+Quantized+Contrastive+Predictive+Coding+for+Template-based+Music+Generation&btnG=",
                    "content_link": "https://arxiv.org/pdf/2004.10120.pdf",
                    "quick_notes": "Not entirely sure what they do in the paper, need to read further. ",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }, 
                {
                    "title": "Infilling Piano Performances",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Infilling+Piano+Performances&btnG=",
                    "content_link": "https://nips2018creativity.github.io/doc/infilling_piano_performances.pdf",
                    "quick_notes": "Magenta group using self-attention methods to 'fill in' pieces of missing music. Useful for computer-assisted composition with HCI",
                    "citation": "",
                    "review_link": "",
                    "website_link": ""
                
                }
            ],
            "Recurrent": [
                {
                    "title": "This time with feeling: learning expressive musical performance",
                    "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=This+time+with+feeling%3A+learning+expressive+musical+performance&btnG=",
                    "content_link": "https://link.springer.com/article/10.1007/s00521-018-3758-9",
                    "quick_notes": "This is a paper that focuses on generating expressive performance using an LSTM network architecture. Haven't read the paper. ",
                    "citation": "Oore, S., Simon, I., Dieleman, S., Eck, D., & Simonyan, K. (2018). This time with feeling: Learning expressive musical performance. Neural Computing and Applications, 1-13.",
                    "review_link": "",
                    "website_link": ""
                
                }
            ]
        }, 
        "Music Information Retrieval": [
            {
                "title": "Music Information Retrieval",
                "google_scholar_link": "https://scholar.google.com/scholar?q=music+information+retrieval&hl=en&as_sdt=0&as_vis=1&oi=scholart",
                "content_link": "http://www.music.mcgill.ca/~ich/classes/mumt611_06/downie_mir_arist37.pdf",
                "quick_notes": "A general introduction and overview of MIR",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Getting Closer to the Essence of Music:The Con Espressione Manifesto",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Getting+Closer+to+the+Essence+of+Music%3AThe+Con+Espressione+Manifesto&btnG=",
                "content_link": "https://arxiv.org/pdf/1611.09733.pdf",
                "quick_notes": "Widmer has been doing research on artificially intelligent computer music based systems for a long time. He has some other papers that I have read through and will probably link here. This is one of his most recent and serves as a sort of encouragment and challenge to the rest of the computer music community and what he thinks the relevant issues and areas of research are worth exploring. If I remember correctly, one of the emphasis he makes is on deep learning and ANN's. ",
                "citation": "Widmer, Gerhard. “Getting Closer to the Essence of Music.” ACM Transactions on Intelligent Systems and Technology 8.2 (2017): 1–13. Crossref. Web.",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "THE WEKINATOR: A SYSTEM FOR REAL-TIME,INTERACTIVE MACHINE LEARNING IN MUSIC",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=THE+WEKINATOR%3A+A+SYSTEM+FOR+REAL-TIME%2C+INTERACTIVE+MACHINE+LEARNING+IN+MUSIC&btnG=",
                "content_link": "https://www.researchgate.net/profile/Perry_Cook3/publication/228719072_The_Wekinator_A_System_for_Real-time_Interactive_Machine_Learning_in_Music/links/00b4953b6fd9d98644000000.pdf",
                "quick_notes": "This is a general system for live MIR statistics on musical performance. I'm not sure if it has been used for any specific application. ",
                "citation": "Fiebrink, R., & Cook, P. R. (2010, August). The Wekinator: a system for real-time, interactive machine learning in music. In Proceedings of The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010)(Utrecht).",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Optical Music Recognition with convolutional sequence-to-sequence models",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=OPTICAL+MUSIC+RECOGNITION+WITH+CONVOLUTIONAL+SEQUENCE-TO-SEQUENCE+MODELS&btnG=",
                "content_link": "https://arxiv.org/pdf/1707.04877.pdf",
                "quick_notes": "OMR is taking an image of a score and producing a digital representation of the score. They use a convolutional model and MusicXML (with MuseScore) to accomplish this. ",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }
        ], 
        "Music Synthesis/Transcription": [
            {
                "title": "Onsets and frames: Dual-objective piano transcription",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Onsets+and+Frames%3A+Dual-Objective+Piano+Transcription&btnG=",
                "content_link": "https://arxiv.org/pdf/1710.11153.pdf",
                "quick_notes": "Use a convolutional and recurrent neural network to create MIDI files from raw audio wav files. Done by Magenta research group",
                "citation": "",
                "review_link": "/research/reviews/onsets-and-frames-dual-objective-piano-transcription/",
                "website_link": "https://magenta.tensorflow.org/onsets-frames"
            
            }, 
            {
                "title": "Wavenet: A generative model for raw audio",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&as_vis=1&q=Wavenet%3A+A+generative+model+for+raw+audio&btnG=",
                "content_link": "https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&utm_medium=email&utm_campaign=Uncubed%20Entry%20%2361%20-%20April%203%2C%202019&utm_term=entry",
                "quick_notes": "An architecture by DeepMind for generating raw wav audio. ",
                "citation": "",
                "review_link": "",
                "website_link": "https://deepmind.com/blog/article/wavenet-generative-model-raw-audio"
            
            }
        ], 
        "Datasets": [
            {
                "title": "THE MAGALOFF CORPUS:AN EMPIRICAL ERROR STUDY",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=THE+MAGALOFF+CORPUS%3A+AN+EMPIRICAL+ERROR+STUDY&btnG=",
                "content_link": "http://depts.washington.edu/icmpc11/ICMPC11/PDF/AUTHOR/MP100017.PDF",
                "quick_notes": "",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            },
            {
                "title": "LEARNING FEATURES OF MUSIC FROM SCRATCH: MusicNET",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=LEARNING+FEATURES+OF+MUSIC+FROM+SCRATCH&btnG=",
                "content_link": "https://arxiv.org/pdf/1611.09827.pdf",
                "quick_notes": "A dataset with raw audio and corresponding labels for note timing and instruments. ",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }
        ],
        "Other": [
            {
                "title": "Attention Is All You Need",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=attention+is+all+you+need&btnG=&oq=attent",
                "content_link": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf",
                "quick_notes": "The paper that introduced the transformer architecture. ",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Bert%3A+Pre-training+of+deep+bidirectional+transformers+for+language+understanding&btnG=",
                "content_link": "https://arxiv.org/pdf/1810.04805.pdf?source=post_elevate_sequence_page---------------------------",
                "quick_notes": "The paper that introduces BERT",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Language Models are Unsupervised Multitask Learners",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Language+Models+are+Unsupervised+Multitask+Learners&btnG=",
                "content_link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
                "quick_notes": "The original paper that introduces GPT-2",
                "citation": "",
                "review_link": "",
                "website_link": "https://openai.com/blog/better-language-models/"
            
            }, 
            {
                "title": "Neural machine translation by jointly learning to align and translate",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=NEURAL+MACHINE+TRANSLATION+BY+JOINTLY+LEARNING+TO+ALIGN+AND+TRANSLATE&btnG=",
                "content_link": "https://arxiv.org/pdf/1409.0473",
                "quick_notes": "The paper that introduces attention as an alternative to RNNs and LSTMs implemented as encoders-decoders",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "ImageNet Classification with Deep Convolutional Neural Networks",
                "google_scholar_link": "https://scholar.google.com/scholar?q=ImageNet+Classification+with+Deep+Convolutional+Neural+Networks&hl=en&as_sdt=0&as_vis=1&oi=scholart",
                "content_link": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
                "quick_notes": "This is the paper that introduces AlexNet, the CNN that won the 2012 ImageNet competition and woke the world up to deep learning. ",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Combining Knowledge-Based and Instance-Based Learning to Exploit Qualitative Knowledge",
                "google_scholar_link": "https://scholar.google.com/scholar?q=Combining+Knowledge-Based+and+Instance-Based+Learning+to+Exploit+Qualitative+Knowledge&hl=en&as_sdt=0&as_vis=1&oi=scholart",
                "content_link": "file:///Users/richardtimpson/Downloads/69-60-PB.pdf",
                "quick_notes": "Pages 371 - 385. An early custom algorithm, named IBL-SMART, developed by Widmer in application to tonal music. Used in his single note expressiveness model",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "Revisiting the Illiac Suite – a rule based approach to stochastic processes",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Revisiting+the+Illiac+Suite+%E2%80%93+a+rule+based+approach+to+stochastic+processes&btnG=",
                "content_link": "http://www.sandred.com/texts/Revisiting_the_Illiac_Suite.pdf",
                "quick_notes": "A revisitation of the illiac suite, which was the first piece of music composed by a computer. ",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }, 
            {
                "title": "MusicXML: An Internet-Friendly Format for Sheet Music",
                "google_scholar_link": "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=MusicXML%3A+An+internet-friendly+format+for+sheet+music&btnG=",
                "content_link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.5431&rep=rep1&type=pdf",
                "quick_notes": "A paper that introduces and provides and overview for the MusicXML format",
                "citation": "",
                "review_link": "",
                "website_link": ""
            
            }
        ]
    }
    
}