---
title: "Resources"
date: 2020-05-05T17:43:09-06:00
draft: false
---

This page is going to act as a central place for me to keep track of all the useful resources that I come across during my research. Right now I have several papers and books saved on my laptop, but it is becoming difficult to keep them organized. I am going to organize all of the references by category with links to the relevant paper, book, or article. 

# Books
* **Deep learning techniques for music generation--a survey** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Deep+Learning+Techniques+for+Music+Generation+%E2%80%93+A+Survey&btnG=)
    * [Content Link](https://arxiv.org/pdf/1709.01620.pdf)
    * Quick Notes: This is an entire book about deep learning for music generation. I've read everything up to the network architectures section. This has been the most informative resource that I have used so far.
    * Citation: Briot, J. P., Hadjeres, G., & Pachet, F. D. (2017). Deep learning techniques for music generation--a survey. arXiv preprint arXiv:1709.01620.
* **Deep Learning Book** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=deep+learning+goodfellow&btnG=)
    * [Content Link](https://github.com/janishar/mit-deep-learning-book-pdf)
    * Quick Notes: The famous book by goodfellow. It is dense enough that I haven't read that much of it, but think that it would be useful to look at the section on RNNs and LSTMs
    * Citation: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.

# Papers
## AI Tutor
* **Toward a High Performance Piano Practice Support System for Beginners**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Toward+a+High+Performance+Piano+Practice+Support+System+for+Beginners&btnG=)
    * [Content Link](http://www.apsipa.org/proceedings/2018/pdfs/0000073.pdf)
    * Quick Notes: This appears to be the closest application to what I want to do. The paper is difficult to read as the english doesn't make much sense in many places (due to the native language of the authors), but the overall idea is as close to what I want to do as anything else I could find. 
    * Citation: Asahi, Shota & Tamura, Satoshi & Sugiyama, Yuko & Hayamizu, Satoru. (2018). Toward a High Performance Piano Practice Support System for Beginners. 73-79. 10.23919/APSIPA.2018.8659463. 
* **Detecting Pianist Hand Posture Mistakes for Virtual Piano Tutoring**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Detecting+Pianist+Hand+Posture+Mistakes+for+Virtual+Piano+Tutoring&btnG=)
    * [Content Link](https://www.researchgate.net/profile/David_Johnson143/publication/318028952_Detecting_Pianist_Hand_Posture_Mistakes_for_Virtual_Piano_Tutoring/links/596042100f7e9b8194fc1119/Detecting-Pianist-Hand-Posture-Mistakes-for-Virtual-Piano-Tutoring.pdf)
    * Quick Notes: This is not 100% relevant to the research that I am doing but is a fascinating idea. It takes a computer vision based approach to tutoring by looking at hand position while playing. This isn't relevant to the research that I am doing but may be in the future. 
    * Citation: Johnson, David & Dufour, Isabelle & Damian, Daniela & Tzanetakis, George. (2016). Detecting Pianist Hand Posture Mistakes for Virtual Piano Tutoring. 
* **Computer-Assisted Musical Instrument Tutoring with Targeted Exercises**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Computer-Assisted+Musical+Instrument+Tutoring+with+Targeted+Exercises&btnG=)
    * [Content Link](http://dspace.library.uvic.ca/bitstream/handle/1828/1081/masters-percival.pdf?sequence=1&isAllowed=y)
    * Quick Notes: This is a masters thesis and talks about computer music approaches for tutoring. It doesn't use any sophisticated deep learning models if I remember but does present some of the problems that are associated with computer based tutoring applications. I haven't read the whole paper in detail, but would like to better understand my own problem that I am trying to solve. 
    * Citation: Percival, G. K. (2008). Computer-assisted musical instrument tutoring with targeted exercises (Doctoral dissertation).
* **Score-Following-in-Practice** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Score-Following-in-Practice&btnG=)
    * [Content Link](https://www.researchgate.net/profile/Cort_Lippe/publication/301649731_Score-Following-in-Practice/links/571fadd508aefa64889a811e.pdf)
    * Quick Notes: An older paper (1992) about score following. Not sure what the computational techniques that they use are
    * Citation: Puckette, M., & Lippe, C. (1992). Score following in practice. In Proceedings of the International Computer Music Conference (pp. 182-182). INTERNATIONAL COMPUTER MUSIC ACCOCIATION.

## Music Analysis and Expresiveness
### General
* **E-LEARNING SOFTWARE FOR IMPROVING STUDENT'S MUSIC PERFORMANCE USING COMPARISONS**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=E-LEARNING+SOFTWARE+FOR+IMPROVING+STUDENT%27S+MUSIC+PERFORMANCE+USING+COMPARISONS&btnG=)
    * [Content Link](https://files.eric.ed.gov/fulltext/ED562302.pdf
    * Quick Notes: This approach analyzes expresiveness by comparing a student piece to a professional. I have read into the paper too much but it is definitely worth looking into. I would probably try to do something similar for my application
* **Expressive Collaborative Music Performance via Machine Learning** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Expressive+Collaborative+Music+Performance+via+Machine+Learning&btnG=)
    * [Content Link](http://reports-archive.adm.cs.cmu.edu/anon/anon/ml2016/CMU-ML-16-103.pdf)
    * Quick Notes: This is a PhD Thesis which explores music performance from a computation perspective. There is quite a bit about musical representation, and he even touches on using computer vision to assist in HCI tools for music performance. The thesis is long, but has a substantial amount of good content
    * Citation: Xia, G. (2016). Expressive Collaborative Music Performance via Machine Learning.
* **A Phylogenetic Approach to Music Performance Analysis** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+Phylogenetic+Approach+to+Music+Performance+Analysis&btnG=)
    * [Content Link](http://www.cs.utexas.edu/users/eladlieb/paper.pdf)
    * Quick Notes: Provides an alternative approach to music expressiveness. Focuses on violin as opposed to piano
    * Citation: Liebman, Elad & Ornoy, Eitan & Chor, Benny. (2012). A Phylogenetic Approach to Music Performance Analysis. Journal of New Music Research. 41. 195-222. 10.1080/09298215.2012.668194. 
* **A survey of computer systems for expressive music performance**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+survey+of+computer+systems+for+expressive+music+performance&btnG=)
    * [Content Link](https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/ComputerSystemsForMusic.pdf)
    * Quick Notes: This is an older paper and forms much of the basis for expressive music performance analysis from a computational perspective. 
    * Citation: Alexis Kirke and Eduardo Reck Miranda. 2009. A survey of computer systems for expressive music performance. ACM Comput. Surv. 42, 1, Article 3 (December 2009), 41 pages. DOI:https://doi.org/10.1145/1592451.1592454
* **On the Potential of Machine Learning for Music Research** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=On+the+Potential+of+Machine+Learning+for+Music+Research&btnG=)
    * [Content Link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.874&rep=rep1&type=pdf)
    * Quick Notes: This is an older paper by widmer where he introduces the idea of using machine learning for music research. I have read a good amount of the paper but need to go back through it to see what the exact algorithms and representation he is using. 
    * Citation: Widmer, G. (2013). On the potential of machine learning for music research. In Readings in Music and Artificial Intelligence (pp. 79-94). Routledge.
* **Overview of the KTH rule system for musical performance**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Overview+of+the+KTH+rule+system+for+musical+performance&btnG=)
    * [Content Link](http://ac-psych.org/en/download-pdf/page/20/volume/2/issue/2/id/15)
    * Quick Notes: This is a popular paper by Friberg that discusses his KTH system for analyzing music expresiveness that he has been developing since the 80's. This doesn't use any sort of data driven approach but uses a set of hardcoded rules that are fine tuned by human feedback to define what expresiveness is. 
    * Citation: Friberg, A., Bresin, R., & Sundberg, J. (2006). Overview of the KTH rule system for musical performance. Advances in cognitive psychology, 2(2-3), 145-161.

### Transformer
* **A BI-DIRECTIONAL TRANSFORMER FOR MUSICAL CHORD RECOGNITION** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=A+BI-DIRECTIONAL+TRANSFORMER+FOR+MUSICAL+CHORD+RECOGNITION&btnG=)
    * [Content Link](https://arxiv.org/pdf/1907.02698.pdf)
    * Review Link: TODO
    * Quick Notes: Uses the self-attention technique for chord recognition and provides an accompanying visualization

## Music Generation
### General
* **Deep Learning for Music Generation - Challenges and Directions** 
    * [Google Scholar Link](https://scholar.google.com/scholar?q=Deep+learning+for+music+generation:+challenges+and+directions&hl=en&as_sdt=0&as_vis=1&oi=scholart)
    * [Content Link](http://www-desir.lip6.fr/~briot/cv/mg-dl-cd-final.pdf)
    * [Review Link]({{<ref "research/reviews/deep-learning-for-music-generation-challenges-and-directions.md">}})
    * Quick Notes: A survey of the current trends in music generation and suggestions of future work

* **Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Enabling+Factorized+Piano+Music+Modeling+and+Generation+with+the+MAESTRO+Dataset&btnG=)
    * [Content Link](https://arxiv.org/pdf/1810.12247.pdf)
    * [Website Link](https://magenta.tensorflow.org/maestro-wave2midi2wave)
    * [Review Link]({{<ref "research/reviews/enabling-factorized-piano-music-modeling-and-generation-with-the-maestro-dataset.md.md">}})
    * Quick Notes: A full audio transcription and generation process proposed by Magenta. They break the problem of generating music audio into 3 parts. 
        * 1st. Audio transcription: Going from Audio to Midi
        * 2nd. Generating new Midi using a Music Transformer
        * 3rd. Generating new audio with the MIDI using the WaveNet architecture. 

### Transformer
* **Music Transformer: Generating music with long-term structure** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Music+transformer%3A+Generating+music+with+long-term+structure&btnG=)
    * [Content Link](https://openreview.net/pdf?id=rJe4ShAcF7)
    * [Website Link](https://magenta.tensorflow.org/music-transformer)
    * [Review Link]({{<ref "/research/reviews/music-transformer-generating-music-with-long-term-structure.md">}})
    * Quick Notes: Magenta's(Google) take on using transformers to model long term relationships in music

* **Music Style Transformer** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Music+Style+Transformer&btnG=)
    * [Content Link](https://csce.ucmss.com/cr/books/2019/LFS/CSREA2019/ICA7029.pdf)
    * [Review Link]({{<ref "/research/reviews/music-style-transformer.md">}})
    * Quick Notes: Applying Magenta's research with a Style Transfer approach

* **Encoding Musical Style with Transformer Autoencoders** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Encoding+Musical+Style+with+Transformer+Autoencoders&btnG=)
    * [Content Link](https://arxiv.org/pdf/1912.05537.pdf)
    * [Website Link](https://magenta.tensorflow.org/transformer-autoencoder)
    * Review Link: TODO
    * Quick Notes: Magenta research group using an autoencoder on top of the music transformer architecture

* **Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Transformer+VAE%3A+A+Hierarchical+Model+for+Structure-Aware+and+Interpretable+Music+Representation+Learning&btnG=)
    * [Content Link](https://ieeexplore-ieee-org.ezproxy.lib.utah.edu/stamp/stamp.jsp?tp=&arnumber=9054554)
    * Review Link: TODO
    * Quick Notes: Another variational auto-encoder with the goal to make create more structure awareness and interpretability in the model.

* **LakhNES: Improving multi-instrumental music generation with cross-domain pre-training** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=LakhNES%3A+Improving+multi-instrumental+music+generation+with+cross-domain+pre-training&btnG=)
    * [Content Link]https://arxiv.org/pdf/1907.04868.pdf()
    * Review Link: TODO
    * Quick Notes: Using the transformer architecture for polyphonic music generation using new data from NES video games system. 

* **Transformer Bard: Music and Poem Generation Using Transformer Models** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Transformer+Bard%3A+Music+and+Poem+Generation+Using+Transformer+Models&btnG=)
    * [Content Link](https://creativecoding.soe.ucsc.edu/courses/cmpm202_w20/_schedule/TransformerBard_202.pdf)
    * Review Link: TODO
    * Quick Notes: Uses the GPT-2 Model for text generation and the Self-Attention Transformer model for music generation

* **Vector Quantized Contrastive Predictive Coding for Template-based Music Generation** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Vector+Quantized+Contrastive+Predictive+Coding+for+Template-based+Music+Generation&btnG=)
    * [Content Link](https://arxiv.org/pdf/2004.10120.pdf)
    * Review Link: TODO
    * Quick Notes: Not entirely sure what they do in the paper, need to read further. 

* **Infilling Piano Performances** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Infilling+Piano+Performances&btnG=)
    * [Content Link](https://nips2018creativity.github.io/doc/infilling_piano_performances.pdf)
    * Review Link: TODO
    * Quick Notes: Magenta group using self-attention methods to 'fill in' pieces of missing music. Useful for computer-assisted composition with HCI

### Recurrent
* **This time with feeling: learning expressive musical performance**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=This+time+with+feeling%3A+learning+expressive+musical+performance&btnG=)
    * [Content Link](https://link.springer.com/article/10.1007/s00521-018-3758-9)
    * Quick Notes: This is a paper that focuses on generating expressive performance using an LSTM network architecture. Haven't read the paper. 
    * Citation: Oore, S., Simon, I., Dieleman, S., Eck, D., & Simonyan, K. (2018). This time with feeling: Learning expressive musical performance. Neural Computing and Applications, 1-13.

## Music Information Retrieval
* **Getting Closer to the Essence of Music:The Con Espressione Manifesto**
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Getting+Closer+to+the+Essence+of+Music%3AThe+Con+Espressione+Manifesto&btnG=)
    * [Content Link](https://arxiv.org/pdf/1611.09733.pdf)
    * Widmer has been doing research on artificially intelligent computer music based systems for a long time. He has some other papers that I have read through and will probably link here. This is one of his most recent and serves as a sort of encouragment and challenge to the rest of the computer music community and what he thinks the relevant issues and areas of research are worth exploring. If I remember correctly, one of the emphasis he makes is on deep learning and ANN's. 
    * Citation: Widmer, Gerhard. “Getting Closer to the Essence of Music.” ACM Transactions on Intelligent Systems and Technology 8.2 (2017): 1–13. Crossref. Web.
* **THE WEKINATOR: A SYSTEM FOR REAL-TIME,INTERACTIVE MACHINE LEARNING IN MUSIC** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=THE+WEKINATOR%3A+A+SYSTEM+FOR+REAL-TIME%2C+INTERACTIVE+MACHINE+LEARNING+IN+MUSIC&btnG=)
    * [Content Link](https://www.researchgate.net/profile/Perry_Cook3/publication/228719072_The_Wekinator_A_System_for_Real-time_Interactive_Machine_Learning_in_Music/links/00b4953b6fd9d98644000000.pdf)
    * Quick Notes: This is a general system for live MIR statistics on musical performance. I'm not sure if it has been used for any specific application. 
    * Citation: Fiebrink, R., & Cook, P. R. (2010, August). The Wekinator: a system for real-time, interactive machine learning in music. In Proceedings of The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010)(Utrecht).

## Music Synthesis/Transcription
* **Onsets and frames: Dual-objective piano transcription** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Onsets+and+Frames%3A+Dual-Objective+Piano+Transcription&btnG=)
    * [Content Link](https://arxiv.org/pdf/1710.11153.pdf)
    * [Website Link](https://magenta.tensorflow.org/onsets-frames)
    * [Review Link]({{<ref "/research/reviews/onsets-and-frames-dual-objective-piano-transcription.md">}})
    * Quick Notes: Use a convolutional and recurrent neural network to create MIDI files from raw audio wav files. Done by Magenta research group

* **Wavenet: A generative model for raw audio** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&as_vis=1&q=Wavenet%3A+A+generative+model+for+raw+audio&btnG=)
    * [Content Link](https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&utm_medium=email&utm_campaign=Uncubed%20Entry%20%2361%20-%20April%203%2C%202019&utm_term=entry)
    * [Website Link](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)
    * Review Link: TODO
    * Quick Notes: An architecture by DeepMind for generating raw wav audio. 

## Other
* **Attention Is All You Need** 
    * [Google Scholar Link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=attention+is+all+you+need&btnG=&oq=attent)
    * [Content Link](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
    * Review Link: TODO
    * Quick Notes: The paper that introduced self-attention and the transformer architecture. 

<!-- Template -->
<!-- * **Title** 
    * [Google Scholar Link]()
    * [Content Link]()
    * Review Link: TODO
    * Quick Notes: -->